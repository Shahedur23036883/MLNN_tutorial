# Understanding Activation Functions in Deep Learning

## Description:  
This tutorial provides an in-depth exploration of activation functions in neural networks, focusing on ReLU, Leaky ReLU, and ELU. It includes theoretical insights, practical implementations, and visualizations using the Fashion-MNIST dataset.

## Feature:
1. Theoretical explanation of ReLU, Leaky ReLU, and ELU with references.
2. Dataset preprocessing for Fashion-MNIST.
3. Model implementation with each activation function.
4. Training, validation, and decision boundary visualizations.
5. Observations and recommendations for using activation functions.


# Setup Instructions:
1. Clone the repository.
2. Install required packages:
   bash
   ```
   pip install tensorflow matplotlib numpy
   ```
   
4. Open the Jupyter Notebook:
   bash
   ```
   jupyter notebook activation_functions_tutorial.ipynb
   ```
   
6. Run each cell sequentially to execute the code and generate visualizations.

# LICENSE
See `LICENSE.txt`
